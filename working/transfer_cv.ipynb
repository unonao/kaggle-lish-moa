{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2Uo4XTGAApR"
   },
   "source": [
    "# base 1d cnn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2OPVQUl3IOT"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = False\n",
    "SEED = [0, 1, 2, 3 ,4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23405,
     "status": "ok",
     "timestamp": 1617943417745,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "JItByrhMG_vS",
    "outputId": "e4e18304-c5fe-4ea9-8aef-72ad1bb16f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n",
      "cwd: /content/drive/MyDrive/git/kaggle-lish-moa/working\n",
      "Collecting iterative-stratification\n",
      "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (1.0.1)\n",
      "Installing collected packages: iterative-stratification\n",
      "Successfully installed iterative-stratification-0.1.6\n"
     ]
    }
   ],
   "source": [
    "#ColaboratoryかKaggleNotebookか判別\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if 'google.colab' in sys.modules:  # colab環境\n",
    "    SEED = [0] # 1つ目のみ\n",
    "    INPUT = Path('/content/input/')\n",
    "\n",
    "    # drive mount\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "    # working dir に移動\n",
    "    os.chdir('/content/drive/MyDrive/git/kaggle-lish-moa/working')\n",
    "    print(\"cwd:\", os.getcwd())\n",
    "    !pip install iterative-stratification\n",
    "\n",
    "elif 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    INPUT = Path('../input/')\n",
    "    sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpsjNrcDAApU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25941,
     "status": "ok",
     "timestamp": 1617943420287,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "bVm66XbyAApU",
    "outputId": "49b00f6d-1958-4c97-bd04-114cf4e04e40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_drug.csv',\n",
       " 'test_features.csv',\n",
       " 'train_targets_scored.csv',\n",
       " 'train_targets_nonscored.csv',\n",
       " 'train_features.csv',\n",
       " 'sample_submission.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../input/lish-moa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_ccT5jfAApV"
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSwNn1fXnioT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55882,
     "status": "ok",
     "timestamp": 1617943450234,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "G000bYeosjuj",
    "outputId": "2784232f-48a6-4a29-9dc0-3882e0f23543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             sig_id  ...  differentiation_inducer\n",
      "0      id_000644bb2  ...                        0\n",
      "1      id_000779bfc  ...                        0\n",
      "2      id_000a6266a  ...                        0\n",
      "3      id_0015fd391  ...                        0\n",
      "4      id_001626bd3  ...                        0\n",
      "...             ...  ...                      ...\n",
      "23809  id_fffb1ceed  ...                        0\n",
      "23810  id_fffb70c0c  ...                        0\n",
      "23811  id_fffc1c3f4  ...                        0\n",
      "23812  id_fffcb9e7c  ...                        0\n",
      "23813  id_ffffdd77b  ...                        0\n",
      "\n",
      "[23814 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# 相関係数の高い nonscored を抽出\n",
    "corr = pd.concat([train_targets_scored, train_targets_nonscored],axis=1).corr()\n",
    "corr[train_targets_nonscored.drop(\"sig_id\",axis=1).columns]\n",
    "corr_se = corr[:len(train_targets_scored.drop(\"sig_id\",axis=1).columns)][train_targets_nonscored.drop(\"sig_id\",axis=1).columns].abs().max(axis=0).sort_values(ascending=False)\n",
    "len(corr_se[corr_se>0.3])\n",
    "non_scored_target_high_corr = list(corr_se[corr_se>0.3].index)\n",
    "train_targets_nonscored_high_corr = train_targets_nonscored[[\"sig_id\"]+non_scored_target_high_corr]\n",
    "print(train_targets_nonscored_high_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2G3y7LeOAApV"
   },
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "feat_dic = {}\n",
    "feat_dic['gene'] = GENES\n",
    "feat_dic['cell'] = CELLS\n",
    "\n",
    "def norm_fit(df_1,saveM = True, sc_name = 'zsco'):   \n",
    "    from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n",
    "    ss_1_dic = {'zsco':StandardScaler(),\n",
    "                'mima':MinMaxScaler(),\n",
    "                'maxb':MaxAbsScaler(), \n",
    "                'robu':RobustScaler(),\n",
    "                'norm':Normalizer(), \n",
    "                'quan':QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),\n",
    "                'powe':PowerTransformer()}\n",
    "    ss_1 = ss_1_dic[sc_name]\n",
    "    df_2 = pd.DataFrame(ss_1.fit_transform(df_1),index = df_1.index,columns = df_1.columns)\n",
    "    if saveM == False:\n",
    "        return(df_2)\n",
    "    else:\n",
    "        return(df_2,ss_1)\n",
    "  \n",
    "def norm_tra(df_1,ss_x):\n",
    "    df_2 = pd.DataFrame(ss_x.transform(df_1),index = df_1.index,columns = df_1.columns)\n",
    "    return(df_2)\n",
    "    \n",
    "# sample norm \n",
    "q2 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['gene']] = (train_features[feat_dic['gene']].T - qmean.values).T\n",
    "q2 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['gene']] = (test_features[feat_dic['gene']].T - qmean.values).T\n",
    "\n",
    "q2 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = train_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "q2 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = test_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "#------------ norm --------------\n",
    "col_num = list(set(feat_dic['gene'] + feat_dic['cell']) )\n",
    "col_num.sort()\n",
    "train_features[col_num], ss = norm_fit(train_features[col_num],True,'quan')\n",
    "test_features[col_num]     = norm_tra(test_features[col_num],ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJsE57NjAApW"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78401,
     "status": "ok",
     "timestamp": 1617943472759,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "NOU5dw-2AApW",
    "outputId": "54a6d742-b98c-422d-af43-227e788b536b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "atp-sensitive_potassium_channel_antagonist      1\n",
       "erbb2_inhibitor                                 1\n",
       "diuretic                                        6\n",
       "autotaxin_inhibitor                             6\n",
       "protein_phosphatase_inhibitor                   6\n",
       "                                             ... \n",
       "serotonin_receptor_antagonist                 404\n",
       "dopamine_receptor_antagonist                  424\n",
       "cyclooxygenase_inhibitor                      435\n",
       "proteasome_inhibitor                          726\n",
       "nfkb_inhibitor                                832\n",
       "Length: 206, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets_scored.sum()[1:].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78396,
     "status": "ok",
     "timestamp": 1617943472759,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "p_kDyJImAApW",
    "outputId": "70356a7b-ac3b-4d02-c692-e40b15967de0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['trt_cp', 'ctl_vehicle'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features['cp_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TPb2MiVAApW"
   },
   "source": [
    "# PCA features + Existing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ER6Qc1dZAApX"
   },
   "outputs": [],
   "source": [
    "# GENES\n",
    "n_comp = 50\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrziePd8AApX"
   },
   "outputs": [],
   "source": [
    "#CELLS\n",
    "n_comp = 15\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74XjzHtXAApY"
   },
   "source": [
    "# feature Selection using Variance Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "executionInfo": {
     "elapsed": 83166,
     "status": "ok",
     "timestamp": 1617943477534,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "6Pr8yv4hAApY",
    "outputId": "0203f577-b283-423e-df51-7bf89f89950a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>...</th>\n",
       "      <th>897</th>\n",
       "      <th>898</th>\n",
       "      <th>899</th>\n",
       "      <th>900</th>\n",
       "      <th>901</th>\n",
       "      <th>902</th>\n",
       "      <th>903</th>\n",
       "      <th>904</th>\n",
       "      <th>905</th>\n",
       "      <th>906</th>\n",
       "      <th>907</th>\n",
       "      <th>908</th>\n",
       "      <th>909</th>\n",
       "      <th>910</th>\n",
       "      <th>911</th>\n",
       "      <th>912</th>\n",
       "      <th>913</th>\n",
       "      <th>914</th>\n",
       "      <th>915</th>\n",
       "      <th>916</th>\n",
       "      <th>917</th>\n",
       "      <th>918</th>\n",
       "      <th>919</th>\n",
       "      <th>920</th>\n",
       "      <th>921</th>\n",
       "      <th>922</th>\n",
       "      <th>923</th>\n",
       "      <th>924</th>\n",
       "      <th>925</th>\n",
       "      <th>926</th>\n",
       "      <th>927</th>\n",
       "      <th>928</th>\n",
       "      <th>929</th>\n",
       "      <th>930</th>\n",
       "      <th>931</th>\n",
       "      <th>932</th>\n",
       "      <th>933</th>\n",
       "      <th>934</th>\n",
       "      <th>935</th>\n",
       "      <th>936</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.126588</td>\n",
       "      <td>0.896743</td>\n",
       "      <td>-0.419975</td>\n",
       "      <td>-0.987724</td>\n",
       "      <td>-0.261362</td>\n",
       "      <td>-1.019833</td>\n",
       "      <td>-1.357432</td>\n",
       "      <td>-0.036391</td>\n",
       "      <td>0.683932</td>\n",
       "      <td>-0.312484</td>\n",
       "      <td>1.544427</td>\n",
       "      <td>0.169884</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>-0.560967</td>\n",
       "      <td>0.284361</td>\n",
       "      <td>-1.063809</td>\n",
       "      <td>-1.140764</td>\n",
       "      <td>0.868345</td>\n",
       "      <td>0.382571</td>\n",
       "      <td>-0.515578</td>\n",
       "      <td>-0.744083</td>\n",
       "      <td>-1.308057</td>\n",
       "      <td>-1.685607</td>\n",
       "      <td>1.232221</td>\n",
       "      <td>0.551543</td>\n",
       "      <td>0.398992</td>\n",
       "      <td>0.238473</td>\n",
       "      <td>0.166220</td>\n",
       "      <td>-0.535662</td>\n",
       "      <td>0.779434</td>\n",
       "      <td>0.414264</td>\n",
       "      <td>-1.121055</td>\n",
       "      <td>-0.059489</td>\n",
       "      <td>-0.445588</td>\n",
       "      <td>-0.204124</td>\n",
       "      <td>0.266637</td>\n",
       "      <td>...</td>\n",
       "      <td>1.874684</td>\n",
       "      <td>0.728876</td>\n",
       "      <td>-0.682201</td>\n",
       "      <td>-0.867972</td>\n",
       "      <td>-0.727414</td>\n",
       "      <td>0.896742</td>\n",
       "      <td>-0.160439</td>\n",
       "      <td>0.201803</td>\n",
       "      <td>-1.701895</td>\n",
       "      <td>0.038630</td>\n",
       "      <td>0.217904</td>\n",
       "      <td>-1.560113</td>\n",
       "      <td>-0.251815</td>\n",
       "      <td>2.059157</td>\n",
       "      <td>0.177284</td>\n",
       "      <td>0.753183</td>\n",
       "      <td>0.172003</td>\n",
       "      <td>0.254249</td>\n",
       "      <td>1.103743</td>\n",
       "      <td>-2.423089</td>\n",
       "      <td>0.282657</td>\n",
       "      <td>-0.056549</td>\n",
       "      <td>-0.550481</td>\n",
       "      <td>0.470953</td>\n",
       "      <td>-0.819566</td>\n",
       "      <td>0.155108</td>\n",
       "      <td>1.129632</td>\n",
       "      <td>1.769502</td>\n",
       "      <td>0.061783</td>\n",
       "      <td>-1.103373</td>\n",
       "      <td>-0.687035</td>\n",
       "      <td>2.363661</td>\n",
       "      <td>-0.119732</td>\n",
       "      <td>1.058464</td>\n",
       "      <td>-0.396841</td>\n",
       "      <td>-0.482863</td>\n",
       "      <td>1.082089</td>\n",
       "      <td>-0.608718</td>\n",
       "      <td>-0.950615</td>\n",
       "      <td>-0.598907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.123960</td>\n",
       "      <td>0.686144</td>\n",
       "      <td>0.281471</td>\n",
       "      <td>0.089220</td>\n",
       "      <td>1.197490</td>\n",
       "      <td>0.695488</td>\n",
       "      <td>0.325732</td>\n",
       "      <td>0.558904</td>\n",
       "      <td>-0.528336</td>\n",
       "      <td>0.846367</td>\n",
       "      <td>-1.255438</td>\n",
       "      <td>-0.563654</td>\n",
       "      <td>-0.200083</td>\n",
       "      <td>0.549301</td>\n",
       "      <td>0.162757</td>\n",
       "      <td>0.402200</td>\n",
       "      <td>0.406731</td>\n",
       "      <td>0.410560</td>\n",
       "      <td>-0.831429</td>\n",
       "      <td>0.511447</td>\n",
       "      <td>1.256254</td>\n",
       "      <td>-0.124842</td>\n",
       "      <td>-0.387620</td>\n",
       "      <td>-0.415918</td>\n",
       "      <td>0.400921</td>\n",
       "      <td>-0.579409</td>\n",
       "      <td>0.651503</td>\n",
       "      <td>0.234805</td>\n",
       "      <td>-0.737432</td>\n",
       "      <td>-0.179216</td>\n",
       "      <td>-0.106861</td>\n",
       "      <td>-0.537304</td>\n",
       "      <td>1.652468</td>\n",
       "      <td>-0.360877</td>\n",
       "      <td>0.367221</td>\n",
       "      <td>-0.269058</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.507209</td>\n",
       "      <td>0.812504</td>\n",
       "      <td>-0.494634</td>\n",
       "      <td>0.511296</td>\n",
       "      <td>0.945359</td>\n",
       "      <td>2.548873</td>\n",
       "      <td>0.652772</td>\n",
       "      <td>-1.680784</td>\n",
       "      <td>-1.395383</td>\n",
       "      <td>2.575009</td>\n",
       "      <td>-0.298309</td>\n",
       "      <td>1.074672</td>\n",
       "      <td>1.706043</td>\n",
       "      <td>-0.366847</td>\n",
       "      <td>-1.130237</td>\n",
       "      <td>1.450744</td>\n",
       "      <td>-0.765327</td>\n",
       "      <td>-0.052048</td>\n",
       "      <td>-0.485376</td>\n",
       "      <td>0.853620</td>\n",
       "      <td>-1.366777</td>\n",
       "      <td>0.516627</td>\n",
       "      <td>0.262665</td>\n",
       "      <td>-0.224431</td>\n",
       "      <td>-1.143572</td>\n",
       "      <td>-0.334881</td>\n",
       "      <td>-1.205319</td>\n",
       "      <td>-0.054506</td>\n",
       "      <td>-0.800481</td>\n",
       "      <td>-0.276697</td>\n",
       "      <td>0.462977</td>\n",
       "      <td>-0.174409</td>\n",
       "      <td>-0.245746</td>\n",
       "      <td>-2.197200</td>\n",
       "      <td>0.679922</td>\n",
       "      <td>0.844831</td>\n",
       "      <td>0.382629</td>\n",
       "      <td>0.042944</td>\n",
       "      <td>0.224284</td>\n",
       "      <td>-0.149290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.791802</td>\n",
       "      <td>0.963103</td>\n",
       "      <td>1.446197</td>\n",
       "      <td>-0.104188</td>\n",
       "      <td>0.016850</td>\n",
       "      <td>1.506677</td>\n",
       "      <td>0.259336</td>\n",
       "      <td>0.388398</td>\n",
       "      <td>0.018690</td>\n",
       "      <td>1.263162</td>\n",
       "      <td>-0.631766</td>\n",
       "      <td>-0.744935</td>\n",
       "      <td>-0.104532</td>\n",
       "      <td>-2.272928</td>\n",
       "      <td>0.915987</td>\n",
       "      <td>-0.516485</td>\n",
       "      <td>0.488828</td>\n",
       "      <td>-0.381854</td>\n",
       "      <td>-0.264876</td>\n",
       "      <td>-0.014108</td>\n",
       "      <td>-0.085669</td>\n",
       "      <td>-0.977670</td>\n",
       "      <td>-1.943257</td>\n",
       "      <td>0.552849</td>\n",
       "      <td>0.644842</td>\n",
       "      <td>0.732406</td>\n",
       "      <td>-1.375768</td>\n",
       "      <td>2.391415</td>\n",
       "      <td>-0.062331</td>\n",
       "      <td>1.604275</td>\n",
       "      <td>-1.456559</td>\n",
       "      <td>0.835065</td>\n",
       "      <td>0.757337</td>\n",
       "      <td>0.233871</td>\n",
       "      <td>0.026929</td>\n",
       "      <td>-1.628269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.397246</td>\n",
       "      <td>-1.556993</td>\n",
       "      <td>-1.364556</td>\n",
       "      <td>-1.276963</td>\n",
       "      <td>-0.575073</td>\n",
       "      <td>1.139321</td>\n",
       "      <td>-2.757371</td>\n",
       "      <td>-0.148768</td>\n",
       "      <td>1.124297</td>\n",
       "      <td>-1.284978</td>\n",
       "      <td>2.288263</td>\n",
       "      <td>3.239749</td>\n",
       "      <td>-3.147330</td>\n",
       "      <td>1.647471</td>\n",
       "      <td>2.900226</td>\n",
       "      <td>0.035748</td>\n",
       "      <td>1.511792</td>\n",
       "      <td>-0.538020</td>\n",
       "      <td>-0.514330</td>\n",
       "      <td>1.722944</td>\n",
       "      <td>-0.042691</td>\n",
       "      <td>-2.499278</td>\n",
       "      <td>-0.361552</td>\n",
       "      <td>-0.065624</td>\n",
       "      <td>-3.439062</td>\n",
       "      <td>0.360763</td>\n",
       "      <td>0.236295</td>\n",
       "      <td>-0.459681</td>\n",
       "      <td>0.782969</td>\n",
       "      <td>-0.706135</td>\n",
       "      <td>0.228319</td>\n",
       "      <td>0.043046</td>\n",
       "      <td>0.128723</td>\n",
       "      <td>0.095275</td>\n",
       "      <td>-0.093033</td>\n",
       "      <td>-1.067634</td>\n",
       "      <td>-0.158550</td>\n",
       "      <td>-0.425903</td>\n",
       "      <td>1.510575</td>\n",
       "      <td>-0.614885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.678723</td>\n",
       "      <td>-0.225107</td>\n",
       "      <td>-0.390217</td>\n",
       "      <td>0.817606</td>\n",
       "      <td>2.347629</td>\n",
       "      <td>-0.829998</td>\n",
       "      <td>-2.240530</td>\n",
       "      <td>0.358999</td>\n",
       "      <td>-0.144604</td>\n",
       "      <td>-1.366325</td>\n",
       "      <td>-0.983661</td>\n",
       "      <td>-0.448022</td>\n",
       "      <td>-1.119056</td>\n",
       "      <td>-0.759329</td>\n",
       "      <td>-1.758028</td>\n",
       "      <td>1.454682</td>\n",
       "      <td>-0.186508</td>\n",
       "      <td>-1.017801</td>\n",
       "      <td>0.333821</td>\n",
       "      <td>-2.112178</td>\n",
       "      <td>2.098888</td>\n",
       "      <td>-1.292435</td>\n",
       "      <td>-1.057987</td>\n",
       "      <td>-1.011599</td>\n",
       "      <td>-1.391260</td>\n",
       "      <td>0.050921</td>\n",
       "      <td>-1.030224</td>\n",
       "      <td>0.311702</td>\n",
       "      <td>-0.397560</td>\n",
       "      <td>-1.224687</td>\n",
       "      <td>-0.954690</td>\n",
       "      <td>0.193184</td>\n",
       "      <td>-2.071169</td>\n",
       "      <td>1.082679</td>\n",
       "      <td>0.848135</td>\n",
       "      <td>-2.098790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.403723</td>\n",
       "      <td>0.799991</td>\n",
       "      <td>-0.846906</td>\n",
       "      <td>-1.825898</td>\n",
       "      <td>0.275374</td>\n",
       "      <td>3.215788</td>\n",
       "      <td>2.466925</td>\n",
       "      <td>2.896389</td>\n",
       "      <td>-1.712297</td>\n",
       "      <td>1.412401</td>\n",
       "      <td>-1.530897</td>\n",
       "      <td>1.101305</td>\n",
       "      <td>-5.110287</td>\n",
       "      <td>-4.281156</td>\n",
       "      <td>1.345197</td>\n",
       "      <td>-1.717697</td>\n",
       "      <td>0.219488</td>\n",
       "      <td>1.444570</td>\n",
       "      <td>2.686697</td>\n",
       "      <td>-0.720024</td>\n",
       "      <td>1.434588</td>\n",
       "      <td>1.635843</td>\n",
       "      <td>0.502135</td>\n",
       "      <td>1.529322</td>\n",
       "      <td>-0.407975</td>\n",
       "      <td>5.915576</td>\n",
       "      <td>7.043598</td>\n",
       "      <td>-1.278447</td>\n",
       "      <td>-3.642887</td>\n",
       "      <td>-0.170505</td>\n",
       "      <td>1.328007</td>\n",
       "      <td>-0.341669</td>\n",
       "      <td>-0.700178</td>\n",
       "      <td>-0.568794</td>\n",
       "      <td>0.617534</td>\n",
       "      <td>-1.144837</td>\n",
       "      <td>2.252996</td>\n",
       "      <td>-0.307845</td>\n",
       "      <td>2.358449</td>\n",
       "      <td>-0.733371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.455668</td>\n",
       "      <td>-0.480522</td>\n",
       "      <td>0.967960</td>\n",
       "      <td>0.978515</td>\n",
       "      <td>1.455881</td>\n",
       "      <td>-0.875480</td>\n",
       "      <td>-0.378600</td>\n",
       "      <td>-0.213123</td>\n",
       "      <td>-1.067782</td>\n",
       "      <td>0.846761</td>\n",
       "      <td>-0.349202</td>\n",
       "      <td>-0.708051</td>\n",
       "      <td>-1.257696</td>\n",
       "      <td>1.176129</td>\n",
       "      <td>0.319188</td>\n",
       "      <td>0.333307</td>\n",
       "      <td>-0.030044</td>\n",
       "      <td>-1.567980</td>\n",
       "      <td>-0.863951</td>\n",
       "      <td>-0.708170</td>\n",
       "      <td>-0.464143</td>\n",
       "      <td>-0.291901</td>\n",
       "      <td>1.055318</td>\n",
       "      <td>0.481658</td>\n",
       "      <td>1.671455</td>\n",
       "      <td>1.008205</td>\n",
       "      <td>1.084597</td>\n",
       "      <td>-0.562678</td>\n",
       "      <td>-1.550207</td>\n",
       "      <td>0.376295</td>\n",
       "      <td>-0.157058</td>\n",
       "      <td>0.842966</td>\n",
       "      <td>1.108839</td>\n",
       "      <td>-0.143759</td>\n",
       "      <td>1.506831</td>\n",
       "      <td>-1.012107</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.842882</td>\n",
       "      <td>0.590315</td>\n",
       "      <td>0.866107</td>\n",
       "      <td>0.838733</td>\n",
       "      <td>0.742959</td>\n",
       "      <td>-0.048764</td>\n",
       "      <td>1.566977</td>\n",
       "      <td>-0.588171</td>\n",
       "      <td>-1.045144</td>\n",
       "      <td>3.052931</td>\n",
       "      <td>-1.138468</td>\n",
       "      <td>-1.484278</td>\n",
       "      <td>0.894380</td>\n",
       "      <td>-2.876091</td>\n",
       "      <td>-0.384195</td>\n",
       "      <td>-0.206759</td>\n",
       "      <td>-2.312379</td>\n",
       "      <td>-0.468270</td>\n",
       "      <td>1.313270</td>\n",
       "      <td>1.193817</td>\n",
       "      <td>0.719543</td>\n",
       "      <td>-0.504175</td>\n",
       "      <td>0.625338</td>\n",
       "      <td>0.591882</td>\n",
       "      <td>0.430694</td>\n",
       "      <td>-0.147733</td>\n",
       "      <td>0.656648</td>\n",
       "      <td>-0.200896</td>\n",
       "      <td>1.242065</td>\n",
       "      <td>-0.432718</td>\n",
       "      <td>-0.425102</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>0.033976</td>\n",
       "      <td>0.282757</td>\n",
       "      <td>0.236529</td>\n",
       "      <td>-0.090623</td>\n",
       "      <td>-0.186010</td>\n",
       "      <td>0.049554</td>\n",
       "      <td>0.686977</td>\n",
       "      <td>0.187832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.187188</td>\n",
       "      <td>-0.034056</td>\n",
       "      <td>-0.249429</td>\n",
       "      <td>-0.832378</td>\n",
       "      <td>-0.679101</td>\n",
       "      <td>0.908953</td>\n",
       "      <td>0.718568</td>\n",
       "      <td>0.480088</td>\n",
       "      <td>-0.637730</td>\n",
       "      <td>-0.055882</td>\n",
       "      <td>0.281132</td>\n",
       "      <td>0.187133</td>\n",
       "      <td>-0.706960</td>\n",
       "      <td>-1.506956</td>\n",
       "      <td>0.370626</td>\n",
       "      <td>-0.778314</td>\n",
       "      <td>0.218043</td>\n",
       "      <td>0.214944</td>\n",
       "      <td>-1.535446</td>\n",
       "      <td>-0.137503</td>\n",
       "      <td>1.132241</td>\n",
       "      <td>-0.510657</td>\n",
       "      <td>0.823143</td>\n",
       "      <td>0.509190</td>\n",
       "      <td>-0.700666</td>\n",
       "      <td>-0.614566</td>\n",
       "      <td>-1.087769</td>\n",
       "      <td>-0.383896</td>\n",
       "      <td>-1.097243</td>\n",
       "      <td>0.279529</td>\n",
       "      <td>0.955677</td>\n",
       "      <td>-0.028320</td>\n",
       "      <td>-0.581780</td>\n",
       "      <td>-0.880376</td>\n",
       "      <td>0.969181</td>\n",
       "      <td>-0.243005</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.756401</td>\n",
       "      <td>-0.818403</td>\n",
       "      <td>0.732983</td>\n",
       "      <td>0.507685</td>\n",
       "      <td>-0.406591</td>\n",
       "      <td>-0.649918</td>\n",
       "      <td>-0.660144</td>\n",
       "      <td>2.410254</td>\n",
       "      <td>-1.417525</td>\n",
       "      <td>-0.057994</td>\n",
       "      <td>2.201000</td>\n",
       "      <td>0.117971</td>\n",
       "      <td>0.400822</td>\n",
       "      <td>2.322524</td>\n",
       "      <td>0.551766</td>\n",
       "      <td>-0.900840</td>\n",
       "      <td>0.855837</td>\n",
       "      <td>-1.174991</td>\n",
       "      <td>0.228284</td>\n",
       "      <td>0.609089</td>\n",
       "      <td>-1.026241</td>\n",
       "      <td>0.148294</td>\n",
       "      <td>2.244669</td>\n",
       "      <td>1.241264</td>\n",
       "      <td>-0.043656</td>\n",
       "      <td>-1.050282</td>\n",
       "      <td>0.709188</td>\n",
       "      <td>-0.058357</td>\n",
       "      <td>-0.780204</td>\n",
       "      <td>1.271030</td>\n",
       "      <td>0.939382</td>\n",
       "      <td>-0.901208</td>\n",
       "      <td>-0.039406</td>\n",
       "      <td>-1.717758</td>\n",
       "      <td>-2.110985</td>\n",
       "      <td>-0.094941</td>\n",
       "      <td>0.872448</td>\n",
       "      <td>-0.179214</td>\n",
       "      <td>0.339059</td>\n",
       "      <td>0.011609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>-1.884473</td>\n",
       "      <td>0.577520</td>\n",
       "      <td>-0.592163</td>\n",
       "      <td>1.313730</td>\n",
       "      <td>-0.988445</td>\n",
       "      <td>0.850154</td>\n",
       "      <td>-0.302785</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>0.635047</td>\n",
       "      <td>0.647216</td>\n",
       "      <td>-0.298722</td>\n",
       "      <td>1.773136</td>\n",
       "      <td>-0.374706</td>\n",
       "      <td>-0.855289</td>\n",
       "      <td>1.187777</td>\n",
       "      <td>0.966057</td>\n",
       "      <td>-0.497439</td>\n",
       "      <td>0.184017</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.193440</td>\n",
       "      <td>-0.300944</td>\n",
       "      <td>-1.103900</td>\n",
       "      <td>0.141793</td>\n",
       "      <td>-1.815007</td>\n",
       "      <td>-0.966702</td>\n",
       "      <td>-1.222007</td>\n",
       "      <td>0.397687</td>\n",
       "      <td>0.179500</td>\n",
       "      <td>0.138097</td>\n",
       "      <td>0.220735</td>\n",
       "      <td>0.236966</td>\n",
       "      <td>0.755984</td>\n",
       "      <td>-0.004636</td>\n",
       "      <td>-0.105144</td>\n",
       "      <td>1.283041</td>\n",
       "      <td>0.547557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673692</td>\n",
       "      <td>1.248552</td>\n",
       "      <td>-1.891382</td>\n",
       "      <td>-0.396722</td>\n",
       "      <td>-0.010035</td>\n",
       "      <td>-1.606604</td>\n",
       "      <td>-0.422678</td>\n",
       "      <td>-1.524455</td>\n",
       "      <td>-1.018971</td>\n",
       "      <td>-0.047771</td>\n",
       "      <td>0.936799</td>\n",
       "      <td>1.223066</td>\n",
       "      <td>-0.410030</td>\n",
       "      <td>0.715196</td>\n",
       "      <td>-0.677887</td>\n",
       "      <td>0.861431</td>\n",
       "      <td>0.573731</td>\n",
       "      <td>0.581244</td>\n",
       "      <td>-0.391945</td>\n",
       "      <td>0.546381</td>\n",
       "      <td>-1.140731</td>\n",
       "      <td>-1.245623</td>\n",
       "      <td>-0.559947</td>\n",
       "      <td>-1.892817</td>\n",
       "      <td>0.162867</td>\n",
       "      <td>-0.440551</td>\n",
       "      <td>-0.393629</td>\n",
       "      <td>2.727819</td>\n",
       "      <td>1.514617</td>\n",
       "      <td>0.884857</td>\n",
       "      <td>2.089866</td>\n",
       "      <td>-0.903363</td>\n",
       "      <td>1.596745</td>\n",
       "      <td>-0.966179</td>\n",
       "      <td>-0.909414</td>\n",
       "      <td>-0.350229</td>\n",
       "      <td>-1.627276</td>\n",
       "      <td>1.369043</td>\n",
       "      <td>-1.987220</td>\n",
       "      <td>0.880771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>ctl_vehicle</td>\n",
       "      <td>48</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.523746</td>\n",
       "      <td>0.629871</td>\n",
       "      <td>0.293035</td>\n",
       "      <td>-1.155001</td>\n",
       "      <td>0.755713</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>-0.332541</td>\n",
       "      <td>0.301984</td>\n",
       "      <td>-0.054996</td>\n",
       "      <td>0.419032</td>\n",
       "      <td>-0.291796</td>\n",
       "      <td>0.074075</td>\n",
       "      <td>0.159425</td>\n",
       "      <td>-0.063017</td>\n",
       "      <td>0.259025</td>\n",
       "      <td>-0.356916</td>\n",
       "      <td>-0.197651</td>\n",
       "      <td>-0.550486</td>\n",
       "      <td>0.821665</td>\n",
       "      <td>-0.726219</td>\n",
       "      <td>0.218768</td>\n",
       "      <td>-0.274455</td>\n",
       "      <td>0.390740</td>\n",
       "      <td>-1.060709</td>\n",
       "      <td>0.632625</td>\n",
       "      <td>1.031457</td>\n",
       "      <td>-0.516732</td>\n",
       "      <td>-1.737759</td>\n",
       "      <td>0.718004</td>\n",
       "      <td>1.550451</td>\n",
       "      <td>-0.332772</td>\n",
       "      <td>-2.226475</td>\n",
       "      <td>-0.474578</td>\n",
       "      <td>-0.033901</td>\n",
       "      <td>-0.385621</td>\n",
       "      <td>-0.309447</td>\n",
       "      <td>...</td>\n",
       "      <td>4.020869</td>\n",
       "      <td>0.882611</td>\n",
       "      <td>-2.952872</td>\n",
       "      <td>-1.611710</td>\n",
       "      <td>0.571437</td>\n",
       "      <td>-2.019117</td>\n",
       "      <td>0.426538</td>\n",
       "      <td>-2.023727</td>\n",
       "      <td>3.154268</td>\n",
       "      <td>-0.598999</td>\n",
       "      <td>-1.030936</td>\n",
       "      <td>2.518993</td>\n",
       "      <td>-0.232341</td>\n",
       "      <td>1.191275</td>\n",
       "      <td>1.262104</td>\n",
       "      <td>-0.391270</td>\n",
       "      <td>0.928126</td>\n",
       "      <td>0.937796</td>\n",
       "      <td>1.476072</td>\n",
       "      <td>-0.571739</td>\n",
       "      <td>0.667097</td>\n",
       "      <td>0.488269</td>\n",
       "      <td>0.758886</td>\n",
       "      <td>1.743447</td>\n",
       "      <td>-1.043349</td>\n",
       "      <td>0.174079</td>\n",
       "      <td>0.821518</td>\n",
       "      <td>1.073441</td>\n",
       "      <td>-0.860698</td>\n",
       "      <td>0.939051</td>\n",
       "      <td>-0.362093</td>\n",
       "      <td>-0.767579</td>\n",
       "      <td>0.407726</td>\n",
       "      <td>0.141635</td>\n",
       "      <td>-0.161827</td>\n",
       "      <td>0.258464</td>\n",
       "      <td>-0.177991</td>\n",
       "      <td>0.050372</td>\n",
       "      <td>0.134395</td>\n",
       "      <td>0.132962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.733179</td>\n",
       "      <td>0.285483</td>\n",
       "      <td>0.339431</td>\n",
       "      <td>0.179894</td>\n",
       "      <td>0.977203</td>\n",
       "      <td>-0.131723</td>\n",
       "      <td>-0.065934</td>\n",
       "      <td>-0.037950</td>\n",
       "      <td>0.427271</td>\n",
       "      <td>-1.271809</td>\n",
       "      <td>0.637745</td>\n",
       "      <td>0.054030</td>\n",
       "      <td>1.676909</td>\n",
       "      <td>0.141191</td>\n",
       "      <td>0.678802</td>\n",
       "      <td>-2.306730</td>\n",
       "      <td>-0.095084</td>\n",
       "      <td>0.724772</td>\n",
       "      <td>0.260592</td>\n",
       "      <td>1.378512</td>\n",
       "      <td>-1.056449</td>\n",
       "      <td>1.255206</td>\n",
       "      <td>0.427376</td>\n",
       "      <td>2.436134</td>\n",
       "      <td>0.517181</td>\n",
       "      <td>0.875201</td>\n",
       "      <td>0.192560</td>\n",
       "      <td>-0.615563</td>\n",
       "      <td>1.151749</td>\n",
       "      <td>-1.824834</td>\n",
       "      <td>-0.457983</td>\n",
       "      <td>-0.596739</td>\n",
       "      <td>1.000482</td>\n",
       "      <td>-0.177672</td>\n",
       "      <td>-0.229582</td>\n",
       "      <td>1.576964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604516</td>\n",
       "      <td>0.958135</td>\n",
       "      <td>-0.576911</td>\n",
       "      <td>3.018175</td>\n",
       "      <td>3.840337</td>\n",
       "      <td>-1.996945</td>\n",
       "      <td>-3.727985</td>\n",
       "      <td>2.913305</td>\n",
       "      <td>0.082226</td>\n",
       "      <td>0.026248</td>\n",
       "      <td>-1.123510</td>\n",
       "      <td>-1.742035</td>\n",
       "      <td>1.632227</td>\n",
       "      <td>-2.704517</td>\n",
       "      <td>1.593665</td>\n",
       "      <td>-0.284915</td>\n",
       "      <td>0.758484</td>\n",
       "      <td>-0.177536</td>\n",
       "      <td>0.690516</td>\n",
       "      <td>0.573094</td>\n",
       "      <td>-2.316931</td>\n",
       "      <td>-0.084358</td>\n",
       "      <td>-0.436005</td>\n",
       "      <td>0.478374</td>\n",
       "      <td>0.263108</td>\n",
       "      <td>-0.916321</td>\n",
       "      <td>0.248089</td>\n",
       "      <td>1.720493</td>\n",
       "      <td>1.079618</td>\n",
       "      <td>1.223236</td>\n",
       "      <td>-0.936273</td>\n",
       "      <td>0.540446</td>\n",
       "      <td>-1.280423</td>\n",
       "      <td>-0.100979</td>\n",
       "      <td>2.306954</td>\n",
       "      <td>-1.543399</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>2.061812</td>\n",
       "      <td>0.069075</td>\n",
       "      <td>0.463491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-1.207260</td>\n",
       "      <td>1.571868</td>\n",
       "      <td>-0.252750</td>\n",
       "      <td>1.119980</td>\n",
       "      <td>-0.490927</td>\n",
       "      <td>-2.094350</td>\n",
       "      <td>-1.591093</td>\n",
       "      <td>1.427559</td>\n",
       "      <td>2.268767</td>\n",
       "      <td>1.273762</td>\n",
       "      <td>2.094853</td>\n",
       "      <td>1.291902</td>\n",
       "      <td>1.892940</td>\n",
       "      <td>1.621346</td>\n",
       "      <td>-0.751478</td>\n",
       "      <td>1.213119</td>\n",
       "      <td>-1.366922</td>\n",
       "      <td>0.049614</td>\n",
       "      <td>-1.761435</td>\n",
       "      <td>-0.049772</td>\n",
       "      <td>-0.241997</td>\n",
       "      <td>-0.936967</td>\n",
       "      <td>-1.865796</td>\n",
       "      <td>-1.273045</td>\n",
       "      <td>-1.182755</td>\n",
       "      <td>-0.705616</td>\n",
       "      <td>-1.252738</td>\n",
       "      <td>-0.416303</td>\n",
       "      <td>0.281581</td>\n",
       "      <td>-0.572523</td>\n",
       "      <td>0.547533</td>\n",
       "      <td>-0.294445</td>\n",
       "      <td>-0.981934</td>\n",
       "      <td>-0.287288</td>\n",
       "      <td>-1.564435</td>\n",
       "      <td>-0.781918</td>\n",
       "      <td>...</td>\n",
       "      <td>2.154809</td>\n",
       "      <td>4.269314</td>\n",
       "      <td>0.338415</td>\n",
       "      <td>4.219887</td>\n",
       "      <td>-2.673192</td>\n",
       "      <td>0.450182</td>\n",
       "      <td>0.327496</td>\n",
       "      <td>0.060720</td>\n",
       "      <td>0.998293</td>\n",
       "      <td>2.410491</td>\n",
       "      <td>-0.600448</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.679609</td>\n",
       "      <td>1.731349</td>\n",
       "      <td>-1.904793</td>\n",
       "      <td>-0.502710</td>\n",
       "      <td>-1.823205</td>\n",
       "      <td>0.370038</td>\n",
       "      <td>-2.247951</td>\n",
       "      <td>0.666404</td>\n",
       "      <td>-1.988196</td>\n",
       "      <td>0.460471</td>\n",
       "      <td>-0.578898</td>\n",
       "      <td>0.576218</td>\n",
       "      <td>-0.557145</td>\n",
       "      <td>4.919924</td>\n",
       "      <td>1.348252</td>\n",
       "      <td>-3.206846</td>\n",
       "      <td>2.013038</td>\n",
       "      <td>-0.234160</td>\n",
       "      <td>3.699281</td>\n",
       "      <td>-0.875680</td>\n",
       "      <td>-0.462016</td>\n",
       "      <td>-0.611243</td>\n",
       "      <td>2.296477</td>\n",
       "      <td>1.880785</td>\n",
       "      <td>-1.981695</td>\n",
       "      <td>0.222117</td>\n",
       "      <td>2.132402</td>\n",
       "      <td>0.643686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 941 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id      cp_type cp_time  ...       934       935       936\n",
       "0      id_000644bb2       trt_cp      24  ... -0.608718 -0.950615 -0.598907\n",
       "1      id_000779bfc       trt_cp      72  ...  0.042944  0.224284 -0.149290\n",
       "2      id_000a6266a       trt_cp      48  ... -0.425903  1.510575 -0.614885\n",
       "3      id_0015fd391       trt_cp      48  ... -0.307845  2.358449 -0.733371\n",
       "4      id_001626bd3       trt_cp      72  ...  0.049554  0.686977  0.187832\n",
       "...             ...          ...     ...  ...       ...       ...       ...\n",
       "23809  id_fffb1ceed       trt_cp      24  ... -0.179214  0.339059  0.011609\n",
       "23810  id_fffb70c0c       trt_cp      24  ...  1.369043 -1.987220  0.880771\n",
       "23811  id_fffc1c3f4  ctl_vehicle      48  ...  0.050372  0.134395  0.132962\n",
       "23812  id_fffcb9e7c       trt_cp      24  ...  2.061812  0.069075  0.463491\n",
       "23813  id_ffffdd77b       trt_cp      72  ...  0.222117  2.132402  0.643686\n",
       "\n",
       "[23814 rows x 941 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(threshold=0.5)\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "train_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83867,
     "status": "ok",
     "timestamp": 1617943478240,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "Pz360sA9AApY",
    "outputId": "f0ffe2f5-d49e-4076-9b00-37b00b04ffad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aldehyde_dehydrogenase_inhibitor' 'antiarrhythmic'\n",
      " 'atm_kinase_inhibitor' 'atp-sensitive_potassium_channel_antagonist'\n",
      " 'autotaxin_inhibitor' 'bacterial_membrane_integrity_inhibitor'\n",
      " 'calcineurin_inhibitor' 'coagulation_factor_inhibitor' 'diuretic'\n",
      " 'elastase_inhibitor' 'erbb2_inhibitor' 'laxative' 'leukotriene_inhibitor'\n",
      " 'lxr_agonist' 'nicotinic_receptor_agonist'\n",
      " 'norepinephrine_reuptake_inhibitor' 'protein_phosphatase_inhibitor'\n",
      " 'retinoid_receptor_antagonist' 'steroid' 'tlr_antagonist'\n",
      " 'tropomyosin_receptor_kinase_inhibitor'\n",
      " 'ubiquitin_specific_protease_inhibitor']\n",
      "(21972, 1180)\n"
     ]
    }
   ],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored_high_corr, on='sig_id')\n",
    "\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "#----\n",
    "# augmentation for less target\n",
    "skew_target_cols = train_targets_scored.drop('sig_id', axis=1).sum()[lambda x:x<10].index.values\n",
    "print(skew_target_cols)\n",
    "augmented_num = 0\n",
    "for col in skew_target_cols:\n",
    "    tmp_df = train[train[col]==1].copy()\n",
    "for _ in range(4):\n",
    "    train = pd.concat((train, tmp_df))\n",
    "    augmented_num += len(tmp_df.index) * 4\n",
    "print(train.shape)\n",
    "train = train.reset_index(drop=True)\n",
    "#---\n",
    "target = train[train_targets_scored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCfMFKR5AApZ"
   },
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "executionInfo": {
     "elapsed": 84270,
     "status": "ok",
     "timestamp": 1617943478647,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "N19wtHx0AApZ",
    "outputId": "eab45146-fcee-4ad7-9515-72d61364377b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>...</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "      <th>abc_transporter_expression_enhancer</th>\n",
       "      <th>dna_methyltransferase_inhibitor</th>\n",
       "      <th>ror_inverse_agonist</th>\n",
       "      <th>nfkb_activator</th>\n",
       "      <th>sars_coronavirus_3c-like_protease_inhibitor</th>\n",
       "      <th>glucocorticoid_receptor_antagonist</th>\n",
       "      <th>macrophage_migration_inhibiting_factor_inhibitor</th>\n",
       "      <th>heme_oxygenase_activators</th>\n",
       "      <th>glutathione_reductase_(nadph)_activators</th>\n",
       "      <th>diacylglycerol_o_acyltransferase_inhibitor</th>\n",
       "      <th>keap1_ligand</th>\n",
       "      <th>steryl_sulfatase_inhibitor</th>\n",
       "      <th>hiv_protease_inhibitor</th>\n",
       "      <th>quorum_sensing_signaling_modulator</th>\n",
       "      <th>camp_stimulant</th>\n",
       "      <th>macrophage_inhibitor</th>\n",
       "      <th>abl_inhibitor</th>\n",
       "      <th>membrane_permeability_inhibitor</th>\n",
       "      <th>ephrin_inhibitor</th>\n",
       "      <th>gaba_gated_chloride_channel_blocker</th>\n",
       "      <th>omega_3_fatty_acid_stimulant</th>\n",
       "      <th>niemann-pick_c1-like_1_protein_antagonist</th>\n",
       "      <th>gap_junction_modulator</th>\n",
       "      <th>dna_dependent_protein_kinase_inhibitor</th>\n",
       "      <th>reducing_agent</th>\n",
       "      <th>big1_inhibitor</th>\n",
       "      <th>tyrosine_phosphatase_inhibitor</th>\n",
       "      <th>hgf_receptor_inhibitor</th>\n",
       "      <th>caspase_inhibitor</th>\n",
       "      <th>selective_estrogen_receptor_modulator_(serm)</th>\n",
       "      <th>imidazoline_ligand</th>\n",
       "      <th>sphingosine_1_phosphate_receptor_agonist</th>\n",
       "      <th>differentiation_inducer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.126588</td>\n",
       "      <td>0.896743</td>\n",
       "      <td>-0.419975</td>\n",
       "      <td>-0.987724</td>\n",
       "      <td>-0.261362</td>\n",
       "      <td>-1.019833</td>\n",
       "      <td>-1.357432</td>\n",
       "      <td>-0.036391</td>\n",
       "      <td>0.683932</td>\n",
       "      <td>-0.312484</td>\n",
       "      <td>1.544427</td>\n",
       "      <td>0.169884</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>-0.560967</td>\n",
       "      <td>0.284361</td>\n",
       "      <td>-1.063809</td>\n",
       "      <td>-1.140764</td>\n",
       "      <td>0.868345</td>\n",
       "      <td>0.382571</td>\n",
       "      <td>-0.515578</td>\n",
       "      <td>-0.744083</td>\n",
       "      <td>-1.308057</td>\n",
       "      <td>-1.685607</td>\n",
       "      <td>1.232221</td>\n",
       "      <td>0.551543</td>\n",
       "      <td>0.398992</td>\n",
       "      <td>0.238473</td>\n",
       "      <td>0.166220</td>\n",
       "      <td>-0.535662</td>\n",
       "      <td>0.779434</td>\n",
       "      <td>0.414264</td>\n",
       "      <td>-1.121055</td>\n",
       "      <td>-0.059489</td>\n",
       "      <td>-0.445588</td>\n",
       "      <td>-0.204124</td>\n",
       "      <td>0.266637</td>\n",
       "      <td>0.381819</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.123960</td>\n",
       "      <td>0.686144</td>\n",
       "      <td>0.281471</td>\n",
       "      <td>0.089220</td>\n",
       "      <td>1.197490</td>\n",
       "      <td>0.695488</td>\n",
       "      <td>0.325732</td>\n",
       "      <td>0.558904</td>\n",
       "      <td>-0.528336</td>\n",
       "      <td>0.846367</td>\n",
       "      <td>-1.255438</td>\n",
       "      <td>-0.563654</td>\n",
       "      <td>-0.200083</td>\n",
       "      <td>0.549301</td>\n",
       "      <td>0.162757</td>\n",
       "      <td>0.402200</td>\n",
       "      <td>0.406731</td>\n",
       "      <td>0.410560</td>\n",
       "      <td>-0.831429</td>\n",
       "      <td>0.511447</td>\n",
       "      <td>1.256254</td>\n",
       "      <td>-0.124842</td>\n",
       "      <td>-0.387620</td>\n",
       "      <td>-0.415918</td>\n",
       "      <td>0.400921</td>\n",
       "      <td>-0.579409</td>\n",
       "      <td>0.651503</td>\n",
       "      <td>0.234805</td>\n",
       "      <td>-0.737432</td>\n",
       "      <td>-0.179216</td>\n",
       "      <td>-0.106861</td>\n",
       "      <td>-0.537304</td>\n",
       "      <td>1.652468</td>\n",
       "      <td>-0.360877</td>\n",
       "      <td>0.367221</td>\n",
       "      <td>-0.269058</td>\n",
       "      <td>0.210515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.791802</td>\n",
       "      <td>0.963103</td>\n",
       "      <td>1.446197</td>\n",
       "      <td>-0.104188</td>\n",
       "      <td>0.016850</td>\n",
       "      <td>1.506677</td>\n",
       "      <td>0.259336</td>\n",
       "      <td>0.388398</td>\n",
       "      <td>0.018690</td>\n",
       "      <td>1.263162</td>\n",
       "      <td>-0.631766</td>\n",
       "      <td>-0.744935</td>\n",
       "      <td>-0.104532</td>\n",
       "      <td>-2.272928</td>\n",
       "      <td>0.915987</td>\n",
       "      <td>-0.516485</td>\n",
       "      <td>0.488828</td>\n",
       "      <td>-0.381854</td>\n",
       "      <td>-0.264876</td>\n",
       "      <td>-0.014108</td>\n",
       "      <td>-0.085669</td>\n",
       "      <td>-0.977670</td>\n",
       "      <td>-1.943257</td>\n",
       "      <td>0.552849</td>\n",
       "      <td>0.644842</td>\n",
       "      <td>0.732406</td>\n",
       "      <td>-1.375768</td>\n",
       "      <td>2.391415</td>\n",
       "      <td>-0.062331</td>\n",
       "      <td>1.604275</td>\n",
       "      <td>-1.456559</td>\n",
       "      <td>0.835065</td>\n",
       "      <td>0.757337</td>\n",
       "      <td>0.233871</td>\n",
       "      <td>0.026929</td>\n",
       "      <td>-1.628269</td>\n",
       "      <td>0.171128</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.678723</td>\n",
       "      <td>-0.225107</td>\n",
       "      <td>-0.390217</td>\n",
       "      <td>0.817606</td>\n",
       "      <td>2.347629</td>\n",
       "      <td>-0.829998</td>\n",
       "      <td>-2.240530</td>\n",
       "      <td>0.358999</td>\n",
       "      <td>-0.144604</td>\n",
       "      <td>-1.366325</td>\n",
       "      <td>-0.983661</td>\n",
       "      <td>-0.448022</td>\n",
       "      <td>-1.119056</td>\n",
       "      <td>-0.759329</td>\n",
       "      <td>-1.758028</td>\n",
       "      <td>1.454682</td>\n",
       "      <td>-0.186508</td>\n",
       "      <td>-1.017801</td>\n",
       "      <td>0.333821</td>\n",
       "      <td>-2.112178</td>\n",
       "      <td>2.098888</td>\n",
       "      <td>-1.292435</td>\n",
       "      <td>-1.057987</td>\n",
       "      <td>-1.011599</td>\n",
       "      <td>-1.391260</td>\n",
       "      <td>0.050921</td>\n",
       "      <td>-1.030224</td>\n",
       "      <td>0.311702</td>\n",
       "      <td>-0.397560</td>\n",
       "      <td>-1.224687</td>\n",
       "      <td>-0.954690</td>\n",
       "      <td>0.193184</td>\n",
       "      <td>-2.071169</td>\n",
       "      <td>1.082679</td>\n",
       "      <td>0.848135</td>\n",
       "      <td>-2.098790</td>\n",
       "      <td>-1.399664</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.455668</td>\n",
       "      <td>-0.480522</td>\n",
       "      <td>0.967960</td>\n",
       "      <td>0.978515</td>\n",
       "      <td>1.455881</td>\n",
       "      <td>-0.875480</td>\n",
       "      <td>-0.378600</td>\n",
       "      <td>-0.213123</td>\n",
       "      <td>-1.067782</td>\n",
       "      <td>0.846761</td>\n",
       "      <td>-0.349202</td>\n",
       "      <td>-0.708051</td>\n",
       "      <td>-1.257696</td>\n",
       "      <td>1.176129</td>\n",
       "      <td>0.319188</td>\n",
       "      <td>0.333307</td>\n",
       "      <td>-0.030044</td>\n",
       "      <td>-1.567980</td>\n",
       "      <td>-0.863951</td>\n",
       "      <td>-0.708170</td>\n",
       "      <td>-0.464143</td>\n",
       "      <td>-0.291901</td>\n",
       "      <td>1.055318</td>\n",
       "      <td>0.481658</td>\n",
       "      <td>1.671455</td>\n",
       "      <td>1.008205</td>\n",
       "      <td>1.084597</td>\n",
       "      <td>-0.562678</td>\n",
       "      <td>-1.550207</td>\n",
       "      <td>0.376295</td>\n",
       "      <td>-0.157058</td>\n",
       "      <td>0.842966</td>\n",
       "      <td>1.108839</td>\n",
       "      <td>-0.143759</td>\n",
       "      <td>1.506831</td>\n",
       "      <td>-1.012107</td>\n",
       "      <td>-1.331879</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21967</th>\n",
       "      <td>id_2c1f61f65</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.052487</td>\n",
       "      <td>-0.090507</td>\n",
       "      <td>0.634989</td>\n",
       "      <td>0.970022</td>\n",
       "      <td>0.622500</td>\n",
       "      <td>0.058178</td>\n",
       "      <td>0.543512</td>\n",
       "      <td>-0.594642</td>\n",
       "      <td>-0.360361</td>\n",
       "      <td>-1.218731</td>\n",
       "      <td>0.958293</td>\n",
       "      <td>0.761964</td>\n",
       "      <td>-0.271883</td>\n",
       "      <td>-0.683985</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.607132</td>\n",
       "      <td>0.853628</td>\n",
       "      <td>-0.138479</td>\n",
       "      <td>0.934118</td>\n",
       "      <td>0.331971</td>\n",
       "      <td>1.218196</td>\n",
       "      <td>0.522146</td>\n",
       "      <td>-0.699850</td>\n",
       "      <td>0.532469</td>\n",
       "      <td>-1.542698</td>\n",
       "      <td>-0.152300</td>\n",
       "      <td>-0.686239</td>\n",
       "      <td>-0.799104</td>\n",
       "      <td>0.505779</td>\n",
       "      <td>1.270769</td>\n",
       "      <td>0.650029</td>\n",
       "      <td>-0.513454</td>\n",
       "      <td>0.267579</td>\n",
       "      <td>0.143805</td>\n",
       "      <td>0.563469</td>\n",
       "      <td>0.368879</td>\n",
       "      <td>0.065039</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21968</th>\n",
       "      <td>id_311b47f39</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.053022</td>\n",
       "      <td>-1.254248</td>\n",
       "      <td>1.023506</td>\n",
       "      <td>-0.701955</td>\n",
       "      <td>0.653108</td>\n",
       "      <td>1.511247</td>\n",
       "      <td>1.466840</td>\n",
       "      <td>0.904672</td>\n",
       "      <td>-1.123266</td>\n",
       "      <td>-1.249615</td>\n",
       "      <td>-1.425165</td>\n",
       "      <td>0.120341</td>\n",
       "      <td>0.176264</td>\n",
       "      <td>-0.592334</td>\n",
       "      <td>-1.659904</td>\n",
       "      <td>1.388189</td>\n",
       "      <td>0.402469</td>\n",
       "      <td>-1.403861</td>\n",
       "      <td>0.312515</td>\n",
       "      <td>1.156923</td>\n",
       "      <td>0.190132</td>\n",
       "      <td>-0.334289</td>\n",
       "      <td>-1.512546</td>\n",
       "      <td>-1.219286</td>\n",
       "      <td>1.489906</td>\n",
       "      <td>0.965432</td>\n",
       "      <td>-0.745090</td>\n",
       "      <td>-0.772918</td>\n",
       "      <td>-0.040477</td>\n",
       "      <td>1.231560</td>\n",
       "      <td>-0.695933</td>\n",
       "      <td>0.976067</td>\n",
       "      <td>-1.158849</td>\n",
       "      <td>1.105856</td>\n",
       "      <td>1.018993</td>\n",
       "      <td>-1.305189</td>\n",
       "      <td>-1.050904</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21969</th>\n",
       "      <td>id_452abbea4</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.274378</td>\n",
       "      <td>0.379436</td>\n",
       "      <td>-0.168918</td>\n",
       "      <td>0.203143</td>\n",
       "      <td>-0.624543</td>\n",
       "      <td>-0.607401</td>\n",
       "      <td>-0.423641</td>\n",
       "      <td>0.337792</td>\n",
       "      <td>-0.127849</td>\n",
       "      <td>0.221964</td>\n",
       "      <td>0.611517</td>\n",
       "      <td>0.070140</td>\n",
       "      <td>0.251288</td>\n",
       "      <td>-0.552554</td>\n",
       "      <td>0.816557</td>\n",
       "      <td>-0.645957</td>\n",
       "      <td>1.025548</td>\n",
       "      <td>0.353395</td>\n",
       "      <td>-0.508629</td>\n",
       "      <td>0.345481</td>\n",
       "      <td>0.703846</td>\n",
       "      <td>-0.855306</td>\n",
       "      <td>-0.694089</td>\n",
       "      <td>0.934408</td>\n",
       "      <td>0.520652</td>\n",
       "      <td>0.520284</td>\n",
       "      <td>-0.502054</td>\n",
       "      <td>-0.609899</td>\n",
       "      <td>0.609651</td>\n",
       "      <td>0.385123</td>\n",
       "      <td>0.634011</td>\n",
       "      <td>0.438970</td>\n",
       "      <td>0.878351</td>\n",
       "      <td>1.195516</td>\n",
       "      <td>-0.725871</td>\n",
       "      <td>-1.578988</td>\n",
       "      <td>0.924955</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21970</th>\n",
       "      <td>id_5c8f28f62</td>\n",
       "      <td>48</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.421358</td>\n",
       "      <td>-0.895211</td>\n",
       "      <td>-0.399550</td>\n",
       "      <td>0.043395</td>\n",
       "      <td>-0.333042</td>\n",
       "      <td>0.693621</td>\n",
       "      <td>-0.440438</td>\n",
       "      <td>-0.715162</td>\n",
       "      <td>0.174992</td>\n",
       "      <td>-0.366629</td>\n",
       "      <td>0.673011</td>\n",
       "      <td>-0.145082</td>\n",
       "      <td>0.186435</td>\n",
       "      <td>-0.277007</td>\n",
       "      <td>0.444210</td>\n",
       "      <td>-0.634385</td>\n",
       "      <td>0.761734</td>\n",
       "      <td>0.423898</td>\n",
       "      <td>2.526040</td>\n",
       "      <td>0.803327</td>\n",
       "      <td>-0.945090</td>\n",
       "      <td>0.315366</td>\n",
       "      <td>-1.372486</td>\n",
       "      <td>0.496475</td>\n",
       "      <td>-0.405035</td>\n",
       "      <td>1.305608</td>\n",
       "      <td>-0.162337</td>\n",
       "      <td>0.420401</td>\n",
       "      <td>0.985519</td>\n",
       "      <td>0.191848</td>\n",
       "      <td>0.928903</td>\n",
       "      <td>0.668194</td>\n",
       "      <td>-0.676193</td>\n",
       "      <td>0.597841</td>\n",
       "      <td>-0.175968</td>\n",
       "      <td>0.640769</td>\n",
       "      <td>-0.702191</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21971</th>\n",
       "      <td>id_908187a04</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.995957</td>\n",
       "      <td>0.183441</td>\n",
       "      <td>1.520053</td>\n",
       "      <td>0.231158</td>\n",
       "      <td>0.690116</td>\n",
       "      <td>0.330630</td>\n",
       "      <td>-0.317990</td>\n",
       "      <td>0.593749</td>\n",
       "      <td>-0.776796</td>\n",
       "      <td>-1.757111</td>\n",
       "      <td>0.599073</td>\n",
       "      <td>-0.307240</td>\n",
       "      <td>2.163615</td>\n",
       "      <td>-1.291395</td>\n",
       "      <td>0.019768</td>\n",
       "      <td>0.351121</td>\n",
       "      <td>-1.228397</td>\n",
       "      <td>-0.824512</td>\n",
       "      <td>-1.060853</td>\n",
       "      <td>-0.319902</td>\n",
       "      <td>-0.984196</td>\n",
       "      <td>-0.199324</td>\n",
       "      <td>-0.093847</td>\n",
       "      <td>-0.668726</td>\n",
       "      <td>0.236486</td>\n",
       "      <td>-0.481063</td>\n",
       "      <td>-0.931414</td>\n",
       "      <td>0.506664</td>\n",
       "      <td>1.027029</td>\n",
       "      <td>1.135127</td>\n",
       "      <td>0.968861</td>\n",
       "      <td>-0.105062</td>\n",
       "      <td>0.286534</td>\n",
       "      <td>0.393074</td>\n",
       "      <td>0.845503</td>\n",
       "      <td>-1.512113</td>\n",
       "      <td>0.775558</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21972 rows × 1179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id  ... differentiation_inducer\n",
       "0      id_000644bb2  ...                       0\n",
       "1      id_000779bfc  ...                       0\n",
       "2      id_000a6266a  ...                       0\n",
       "3      id_0015fd391  ...                       0\n",
       "4      id_001626bd3  ...                       0\n",
       "...             ...  ...                     ...\n",
       "21967  id_2c1f61f65  ...                       0\n",
       "21968  id_311b47f39  ...                       0\n",
       "21969  id_452abbea4  ...                       0\n",
       "21970  id_5c8f28f62  ...                       0\n",
       "21971  id_908187a04  ...                       0\n",
       "\n",
       "[21972 rows x 1179 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eF-HOkGjAApZ"
   },
   "outputs": [],
   "source": [
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFu4cSyJAApZ"
   },
   "source": [
    "# CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 871,
     "status": "ok",
     "timestamp": 1617943909311,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "FQx5FrHL-M15",
    "outputId": "bbaa56e1-647e-44fc-c4de-88ca34c5772f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       NaN\n",
       "1       NaN\n",
       "2       NaN\n",
       "3       NaN\n",
       "4       NaN\n",
       "         ..\n",
       "21967   NaN\n",
       "21968   NaN\n",
       "21969   NaN\n",
       "21970   NaN\n",
       "21971   NaN\n",
       "Name: kfold, Length: 21972, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuzWJHIeAApZ"
   },
   "outputs": [],
   "source": [
    "train_drug = pd.read_csv('../input/lish-moa/train_drug.csv')\n",
    "\n",
    "folds = train.copy()\n",
    "def make_fold():\n",
    "    folds = train.copy()\n",
    "    \"\"\"\n",
    "    # LOCATE DRUGS\n",
    "    vc = train_drug.drug_id.value_counts()\n",
    "    vc1 = vc.loc[vc <= 19].index\n",
    "    vc2 = vc.loc[vc > 19].index\n",
    "\n",
    "\n",
    "    # kfold - leave drug out target2 = target.copy()\n",
    "    dct1 = {}; dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values}\n",
    "        dct1.update(dd)\n",
    "\n",
    "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    target2['kfold'] = target2.drug_id.map(dct1)\n",
    "    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n",
    "    target2.kfold = target2.kfold.astype(int)\n",
    "    folds['kfold'] = target2['kfold'].copy()\n",
    "\n",
    "    \"\"\"\n",
    "    folds = train.copy()\n",
    "\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5)\n",
    "\n",
    "    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n",
    "        folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "    folds['kfold'] = folds['kfold'].astype(int)\n",
    "    folds\n",
    "\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1617943965280,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "pRQVceWyAApa",
    "outputId": "91b8761e-ef83-4ed0-8f37-16c6e3a052f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21972, 1179)\n",
      "(21972, 1180)\n",
      "(3624, 940)\n",
      "(21972, 207)\n",
      "(3982, 207)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWdcFBiZAApa"
   },
   "source": [
    "# Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQASMbDeAApa"
   },
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWcfeDeHAApb"
   },
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnOMpFByAApb"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRLTrUg_AApc"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        cha_1 = 256\n",
    "        cha_2 = 512\n",
    "        cha_3 = 512\n",
    "\n",
    "        cha_1_reshape = int(hidden_size/cha_1)\n",
    "        cha_po_1 = int(hidden_size/cha_1/2)\n",
    "        cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
    "\n",
    "        self.cha_1 = cha_1\n",
    "        self.cha_2 = cha_2\n",
    "        self.cha_3 = cha_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
    "        self.dropout_c1 = nn.Dropout(0.1)\n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
    "\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2 = nn.Dropout(0.1)\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "        self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2_1 = nn.Dropout(0.3)\n",
    "        self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "        self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n",
    "        self.dropout_c2_2 = nn.Dropout(0.2)\n",
    "        self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.celu(self.dense1(x), alpha=0.06)\n",
    "\n",
    "        x = x.reshape(x.shape[0],self.cha_1,self.cha_1_reshape)\n",
    "\n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = self.dropout_c1(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x_s = x\n",
    "\n",
    "        x = self.batch_norm_c2_1(x)\n",
    "        x = self.dropout_c2_1(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "\n",
    "        x = self.batch_norm_c2_2(x)\n",
    "        x = self.dropout_c2_2(x)\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x =  x * x_s\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "\n",
    "        x = self.flt(x)\n",
    "\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n",
    "                                                      pos_weight = pos_weight)\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbtL6TerAApc"
   },
   "source": [
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pR9l3N4dAApd"
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1617943968354,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "Ch3qb7wHAApd",
    "outputId": "97214951-c206-4509-c0c2-c949539292f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in non_scored_target_high_corr]\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9Wlu3taAApd"
   },
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "hidden_size=4096\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33d8fNcNAApe"
   },
   "source": [
    "# Single fold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1617943969914,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "mEIIi7HY0G5V",
    "outputId": "b4d39506-823e-4b19-e5cd-d161adb7e969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21972, 1183)\n",
      "(3624, 943)\n"
     ]
    }
   ],
   "source": [
    "print(process_data(folds).shape)\n",
    "print(process_data(test).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEMLld4cAApe"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    #----\n",
    "    # pretrain non_scored_target_high_corr\n",
    "    print(\"pretrain \")\n",
    "    print(\" start\")\n",
    "    loss_fr = nn.BCEWithLogitsLoss()\n",
    "    loss_va = nn.BCEWithLogitsLoss()    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=len(non_scored_target_high_corr),\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    x_train, y_train  = train_df[feature_cols].values, train_df[non_scored_target_high_corr].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[non_scored_target_high_corr].values\n",
    "    \n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE*0.1, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5,max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_fr, trainloader, DEVICE)\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "\n",
    "    model.dense3 = nn.utils.weight_norm(nn.Linear(model.cha_po_2, num_targets))\n",
    "    model.to(DEVICE)\n",
    "    #----\n",
    "\n",
    "\n",
    "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    \n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,   max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "    loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n",
    "    loss_va = nn.BCEWithLogitsLoss()    \n",
    "\n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "    \n",
    "    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer, scheduler, loss_fr, trainloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAFXT5sYAApe"
   },
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEi192YtAApf",
    "outputId": "64b33dd9-69cf-4caa-89ed-ba9bc5a37eca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain \n",
      " start\n",
      "FOLD: 0, EPOCH: 0,train_loss: 0.736991957909819, valid_loss: 0.7079861981528146\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.46732067677151895\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.0238169651478529\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.020228623007626637\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.018297283085329193\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.018243167799074148\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.018094169641179696\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.017344260196862877\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.017836965726954597\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.01687898152116416\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.01765461739684854\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.016783716494514458\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.019144801050424577\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.01676632184535265\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.017755941674113273\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.016853208464664825\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.017584330295877797\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.01672674286975593\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.017577969415911606\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.016752655415431313\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.01768881550856999\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.01666151381948072\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.017992178350687026\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.016653308192726927\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.017602379087890897\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.01650552955739524\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.01766886745712587\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.016415257330821907\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.017494180665484495\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.016210112788215065\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.017379269190132617\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01600128740909091\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.01727870000260217\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.015788745512996895\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.017232554193053928\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.015530023359410141\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.017341333494654724\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.015079843305537233\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.01718688290566206\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.01458487754849636\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.01723023394920996\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.014025260626837828\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.01725199640329395\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.013329349775407194\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.01727865601756743\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.012610716394324234\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.01729638156081949\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.011959896125979181\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.01730627449495452\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.011609684412732073\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.017309936136007308\n",
      "pretrain \n",
      " start\n",
      "FOLD: 1, EPOCH: 0,train_loss: 0.7372016019194666, valid_loss: 0.7074548469649421\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.46769495576239417\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.02549949112451739\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.020086193742760775\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.019381649636973936\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.01800401521044491\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.01922841416671872\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.01706041943581\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.019151742414881785\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.016847464928988122\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.01925059563169877\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.016603791821122604\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.019196113338693976\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.016566287984486915\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.018910788517031405\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.01662113992021467\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.019184378596643608\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.016588104848008958\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.019046173430979252\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.016516932972917592\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.01899948162544105\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.016561850293600647\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.019315252287520304\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.01648426470584678\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.01881639294636746\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.016316447692522166\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.018994296409396663\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.016260174612929352\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.019046584661636088\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.016063527312863916\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.018772366715388164\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.0158468554761723\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.01879648515023291\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.015557306709896475\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.018614241465305287\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.015284965718912817\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.018827730556949973\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.01487966731571368\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.018531088665541675\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.014315112799841122\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.01886576796985335\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.013739326105446276\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.018820196726462908\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.01300492347048147\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.018614834148643747\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01217924988400327\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.01869068038649857\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.011486472725106852\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.018685881772802934\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.011128653629417838\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.018673709076311853\n",
      "pretrain \n",
      " start\n",
      "FOLD: 2, EPOCH: 0,train_loss: 0.7369479722734811, valid_loss: 0.7088243470472448\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.46753869485110044\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.023882512770154896\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.020445484573534435\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.018574971212622\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.018200948766932106\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.017849058515447026\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.01722865282436428\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.01779646601747064\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.016776660922914743\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.01776914588887902\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.016725358094318188\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.018391633132363066\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.01669354572136333\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.01749202222837245\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.016773882578464523\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.01768834439708906\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.016650191096561975\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.017853913801338744\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.01667666995384987\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.01759384320501019\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.016617678584989862\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.017751923707478186\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.016545186426652515\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.01729837229804081\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.016422976550740608\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.017497749615679768\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.016262803894832083\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.017397859352914727\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.01616300440703829\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.01756856979473549\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.015987519896926657\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.01740802323226543\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.01570789011168307\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.017166104934671345\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.015420649504370016\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.016995072611333692\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.014998229308242815\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.01716638041441055\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.014516398418640745\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.017196754021022248\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.013961526693479307\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.017072024344302276\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.013210701749430618\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.017087162718834245\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.012461859004005142\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.017128861591439035\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.0117782444720143\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.017142353415050927\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.011449717876055965\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.0171675752464901\n",
      "pretrain \n",
      " start\n",
      "FOLD: 3, EPOCH: 0,train_loss: 0.737115189217139, valid_loss: 0.7045762453760419\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.46777594584863685\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.023625228873320987\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.020461740552623203\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.018712373397180012\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.018258841580076925\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.018016491617475237\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.017154403658502775\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.017776673845946788\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.016878801343989544\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.017722236524735178\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.01674045278839227\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.01782736602638449\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.016644158400595188\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.017957943065890245\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.016678257082737444\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.017732917038457734\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.01672616321593523\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.01759389199848686\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.016589457317646862\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.01775747471089874\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.016610054157512343\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.01771803019302232\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.016484724317231903\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.0177237850480846\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.01644638473626928\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.01761688877429281\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.01634674483532275\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.01756297145038843\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.01609735744262951\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.01748205747987543\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.015930463411453842\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.01756033351910966\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.015688334153020296\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.017273726207869395\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.015328680678014305\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.01730685808828899\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.015031170099973679\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.01726983633956739\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.014446539392667835\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.017195595721048968\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.013891236687861923\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.017063754743763378\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.013169034316703894\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.017221793638808387\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.012441226078764252\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.017301288859120436\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.011774592560486517\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.017329361555831774\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.011442492823993814\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.017328025666730744\n",
      "pretrain \n",
      " start\n",
      "FOLD: 4, EPOCH: 0,train_loss: 0.7372402674046116, valid_loss: 0.7077060750552586\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.46696462553750345\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.024218183383345603\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.02025588051132534\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.01878426820039749\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.017801978571367436\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.01811692150575774\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.017182741867567318\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.017942776158452033\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.016679017406388903\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.017848816434187547\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.016577382160323686\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.017846304976514408\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.01662042940143442\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.01800131680709975\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.01661140984599141\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.017751459245170867\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.01662356583941458\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.01806313871805157\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.016521247751686886\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.01777088926838977\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.016505436634347923\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.01778086413230215\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.016483269652108782\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.017788120252745492\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.016390230648381555\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.01762766744941473\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.016200588773126186\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.017705261893570422\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.01604759737012395\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.017505512971963202\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.015821758866904005\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.01770193502306938\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.015618844450437937\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.017759891838899682\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.015254985082192698\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.017482104285487108\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.014853335073406713\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.017405635597450394\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.014331895122439533\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.017344582879117557\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.013651673533562302\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.017293582084987845\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.012875281206831552\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.01738301669912679\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.012067283537454796\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.017449323460459708\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.011301673739554657\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.017461279008005346\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.010948866261574238\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.01747848495308842\n"
     ]
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "for seed in SEED:\n",
    "    folds = make_fold()\n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1617944558317,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "lelm_JG78wUr",
    "outputId": "671e2724-4e8b-4005-f18c-053a215cf808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23814\n",
      "23814\n",
      "CV log_loss:  0.016161400758170064\n"
     ]
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(train.iloc[:-augmented_num,:][['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "print(len(train_targets_scored))\n",
    "print(len(valid_results))\n",
    "\n",
    "y_true = train_targets_scored[target_cols].values\n",
    "y_pred = valid_results[target_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1617944558305,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "lBiGNax5AApg"
   },
   "outputs": [],
   "source": [
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1617944558317,
     "user": {
      "displayName": "村上直輝",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giraof9EDeJtiW4D7T2InMuv2ZFB3L0rKZfB-IEag=s64",
      "userId": "14275386975613676415"
     },
     "user_tz": -540
    },
    "id": "TDukB317AApg",
    "outputId": "67e0bd93-fee1-42f4-d55a-a9e7a1e4db0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3982, 207)"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzE5qxyf0G3W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_cv.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
